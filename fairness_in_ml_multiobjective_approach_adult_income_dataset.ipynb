{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from pymoo.core.problem import Problem\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.operators.selection.tournament import TournamentSelection\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.termination import get_termination\n",
    "\n",
    "from pathlib import Path\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(Path.cwd())\n",
    "file_path = path / 'dataset' / 'adult_income.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
    "                'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n",
    "                'hours_per_week', 'native_country', 'income']\n",
    "\n",
    "df = pd.read_csv(file_path, names=column_names, sep=',', skipinitialspace=True)\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df['income'] = df['income'].apply(lambda x: 1 if x == '>50K' else 0)\n",
    "X = df.drop('income', axis=1)\n",
    "y = df['income']\n",
    "\n",
    "categorical_columns = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "continuous_columns = [col for col in X.columns if col not in categorical_columns]\n",
    "\n",
    "X = pd.get_dummies(X, columns=categorical_columns)\n",
    "one_hot_encoded_columns = [col for col in X.columns if col not in continuous_columns]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_continuous = pd.DataFrame(scaler.fit_transform(X[continuous_columns]), index=X.index, columns=continuous_columns)\n",
    "X = X_continuous.join(X[one_hot_encoded_columns])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_calculator(protected_attribute_list, fairness_metric, X, y, y_pred):\n",
    "    fairness_dictionary = {}\n",
    "\n",
    "    for protected_attribute in protected_attribute_list:\n",
    "        protected_attribute_index = X.columns.get_loc(protected_attribute)\n",
    "        subset_y = []\n",
    "        subset_y_pred = []\n",
    "        indices = X.index[X.iloc[:, protected_attribute_index] == 1].to_list() \n",
    "        \n",
    "        for i in indices:\n",
    "            subset_y.append(y[i])\n",
    "            subset_y_pred.append(y_pred[i])\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(subset_y, subset_y_pred).ravel()\n",
    "        \n",
    "        if fairness_metric == 'Equal Opportunity':\n",
    "            tpr = tp / (tp + fn)\n",
    "            fairness = tpr\n",
    "        elif fairness_metric == 'Equalized Odds':\n",
    "            fpr = fp / (fp + tn)\n",
    "            fairness = fpr\n",
    "        elif fairness_metric == 'Disparate Impact':\n",
    "            n = tn + fp + fn + tp\n",
    "            ppp = (tp + fp / n)\n",
    "            fairness = ppp\n",
    "      \n",
    "        fairness_dictionary[protected_attribute] = fairness\n",
    "\n",
    "    fairness_difference = abs(fairness_dictionary[protected_attribute_list[0]] - fairness_dictionary[protected_attribute_list[1]])\n",
    "    return fairness_difference\n",
    "\n",
    "def lexicographic_tournament(pop, P, threshold, priority, **kwargs):\n",
    "    n_tournaments, n_competitors = P.shape\n",
    "    if priority == 1:\n",
    "        objective_1_index = 0\n",
    "        objective_2_index = 1\n",
    "    elif priority == 2:\n",
    "        objective_1_index = 1\n",
    "        objective_2_index = 0\n",
    "    else:\n",
    "        print('ERROR')\n",
    "\n",
    "    if n_competitors != 2:\n",
    "        raise Exception(\"Only pressure=2 allowed for lexicographic tournament!\")\n",
    "\n",
    "    S = np.full(n_tournaments, -1, dtype=int)\n",
    "\n",
    "    for i in range(n_tournaments):\n",
    "        x, y = P[i]\n",
    "\n",
    "        a_x = pop[x].F[objective_1_index]\n",
    "        a_y = pop[y].F[objective_1_index]\n",
    "        t_a = threshold[objective_1_index]\n",
    "\n",
    "        if abs(a_x - a_y) > t_a:\n",
    "            S[i] = x if a_x < a_y else y\n",
    "\n",
    "        else:\n",
    "            b_x = pop[x].F[objective_2_index]\n",
    "            b_y = pop[y].F[objective_2_index]\n",
    "            t_b = threshold[objective_2_index]\n",
    "\n",
    "            if abs(b_x - b_y) < t_b:\n",
    "                S[i] = x if b_x < b_y else y\n",
    "\n",
    "            else:\n",
    "                S[i] = x if a_x < a_y else y\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(lr, protected_attribute_list, fairness_metric):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=float(lr)), metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "    y_pred = model.predict(X_train)\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_pred = pd.Series(y_pred, index=X_train.index)\n",
    "\n",
    "    fairness_difference = fairness_calculator(protected_attribute_list, fairness_metric, X_train, y_train, y_pred)\n",
    "\n",
    "    return accuracy, fairness_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateOptimizationProblem(Problem):\n",
    "    def __init__(self, protected_attribute_list, fairness_metric):\n",
    "        super().__init__(n_var=1, n_obj=2, n_constr=0, xl=0.0001, xu=0.1)\n",
    "        self.protected_attribute_list = protected_attribute_list\n",
    "        self.fairness_metric = fairness_metric\n",
    "\n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        results = np.array([train_and_evaluate(lr, self.protected_attribute_list, self.fairness_metric) for lr in x])\n",
    "        negative_accuracy = -results[:, 0]\n",
    "        fairness_diference = results[:, 1]\n",
    "        out[\"F\"] = np.column_stack([negative_accuracy, fairness_diference])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting combination: 0\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 684us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 921us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 832us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 732us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 872us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 878us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 839us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 796us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 896us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 899us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 983us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 854us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 983us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 988us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 893us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 906us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 888us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 956us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 838us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 915us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 780us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 897us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 873us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 898us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 964us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 907us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 859us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 866us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 904us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 817us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 773us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 699us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 650us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 666us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 897us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 734us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 797us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 855us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 995us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 740us/step\n",
      "==========================================================\n",
      "n_gen  |  n_eval  | n_nds  |      eps      |   indicator  \n",
      "==========================================================\n",
      "     1 |       50 |      2 |             - |             -\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 871us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step  \n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 827us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 786us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 755us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 637us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 685us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 812us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 740us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 697us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 805us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 761us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 666us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 937us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 837us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 767us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 649us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 674us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 629us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 664us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 731us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 821us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 806us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 767us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 981us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 759us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 657us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 625us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 875us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 744us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 748us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 834us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 816us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 764us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714us/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m755/755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore', category=UserWarning, module='keras')\n",
    "population_size = 50\n",
    "number_generations = 10\n",
    "priority = 1 #accuracy\n",
    " \n",
    "fairness_metric_vector = ['Equal Opportunity', 'Equalized Odds', 'Disparate Impact']\n",
    "protected_attribute_vector = ['Sex', 'Race']\n",
    "threshold = [1e-3, 1e-1, 10]\n",
    "threshold_vector = list(itertools.product(threshold, threshold))\n",
    "results_dictionary = {}\n",
    "\n",
    "complete_combinations = list(itertools.product(fairness_metric_vector, protected_attribute_vector, threshold_vector))\n",
    "\n",
    "start = 0 #state\n",
    "for index in range(start, len(complete_combinations)):\n",
    "    print (f'Starting combination: {index}')\n",
    "\n",
    "    combination = complete_combinations[index]\n",
    "    fairness_metric = combination[0]\n",
    "    protected_attribute = combination[1]\n",
    "    threshold = combination[2]\n",
    "\n",
    "    if protected_attribute == 'Sex':\n",
    "        protected_attribute_list = ['sex_Male', 'sex_Female'] \n",
    "    elif protected_attribute == 'Race':\n",
    "        protected_attribute_list = ['race_White', 'race_Black']\n",
    "    else:\n",
    "        print('ERROR')\n",
    "    \n",
    "    \n",
    "    \n",
    "    selection = TournamentSelection(pressure=2, func_comp=lambda pop, P, **kwargs: lexicographic_tournament(pop, P, threshold, priority, **kwargs))\n",
    "    problem = LearningRateOptimizationProblem(protected_attribute_list=protected_attribute_list, fairness_metric=fairness_metric)\n",
    "    algorithm = NSGA2(pop_size=population_size, eliminate_duplicates=True, selection=selection)\n",
    "    \n",
    "    res = minimize(problem,\n",
    "                algorithm,\n",
    "                ('n_gen', number_generations),\n",
    "                seed=1,\n",
    "                verbose=True)\n",
    "\n",
    "    solutions = res.F.tolist()\n",
    "    learning_rates = res.X.tolist()\n",
    "\n",
    "    title = f'Pareto Front between negative accuracy and {fairness_metric} Difference regarding {protected_attribute}'\n",
    "    xlabel = 'Negative accuracy'\n",
    "    ylabel = f'{fairness_metric} Difference regarding {protected_attribute}'\n",
    "    name = f'{fairness_metric}_{protected_attribute}_{str(threshold)}'\n",
    "\n",
    "    results_dictionary[name] = {\n",
    "        'priority': priority,\n",
    "        'population size': population_size,\n",
    "        'number generations': number_generations,\n",
    "\n",
    "        'threshold fairness difference': threshold[0],\n",
    "        'threshold negative accuracy': threshold[1],\n",
    "        'fairness metric': fairness_metric,\n",
    "        'protected attribute': protected_attribute,\n",
    "        \n",
    "        'solutions': solutions,\n",
    "        'learning rate': learning_rates\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(results_dictionary).T \n",
    "    df.to_excel('results.xlsx', index=True)\n",
    "\n",
    "    print(\"Pareto front solutions (Fairness, Negative Accuracy):\")\n",
    "    for f in solutions:\n",
    "        print(f)\n",
    "    print(\"Corresponding learning rates:\")\n",
    "    for x in learning_rates:\n",
    "        print(x)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(res.F[:, 0], res.F[:, 1], c='blue', marker='o')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    file_out = path / 'images' / 'adult_income_dataset' / f'{name}.png'\n",
    "    plt.savefig(file_out, dpi=300)\n",
    "    plt.show()\n",
    "    print (f'Finishing combination: {index}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
